{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c9A-Z8IFe04S"
   },
   "outputs": [],
   "source": [
    "#put the entire song into a single string\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "data = \"In the town of Athy one Jeremy Lanigan \\n Battered away ... ...\"\n",
    "\n",
    "corpus = data.lower().split(\"\\n\")\n",
    "\n",
    "tokenizer.fit_on_texts(corpus)# generate key-value pair, key  word, value is token\n",
    "total_words = len(tokenizer.word_index) + 1 # 1 is out of vocabulary text\n",
    "\n",
    "\n",
    "#*******************************************\n",
    "# Turn into Training Data\n",
    "#*******************************************\n",
    "\n",
    "input_sequences = []\n",
    "for line in corpus: \n",
    "  token_list = tokenizer.texts_to_sequences([line])[0] #only one sentences, [0] to get that sentences\n",
    "  for i in range(1, len(token_list)):\n",
    "    n_gram_sequence = token_list[:i+1]\n",
    "    input_sequences.append(n_gram_sequence)\n",
    "\n",
    "max_sequence_len = max(len(x) for x in input_sequences) # find the longest sentences in input_sequences\n",
    "#pad all sequences so that they are the same length\n",
    "#by using pre-padding, it is simple to get the last token by grabbing the last word\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen = max_sequence_len, padding = 'pre'))\n",
    "\n",
    "\n",
    "#turn into label, take all but the last character -> x, last character -> y\n",
    "xs = input_sequences[:,:-1]\n",
    "labels = input_sequences[:,-1]\n",
    "\n",
    "#to one hot encode my label, convert from a list to categorical. \n",
    "#e.g. labels = [70], ys index 70 is set to 1, other else is 0\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes = total_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vHvsjCPMhA8q"
   },
   "source": [
    "token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\n",
    "will convert line ot text like \"*In the town of Athy one Jeremy Lanigan*\" to list of token represents words [4, 2 , 66, 8, 67, 68, 69, 70]  \n",
    "\n",
    "The result will be \n",
    "\n",
    "\n",
    "| Line|Input Sequences |\n",
    "|:----:|:----|\n",
    "| [4, 2 , 66, 8, 67, 68, 69, 70]   |   [4,2]  | \n",
    "|    | [4, 2, 66]  |\n",
    "|    | [4, 2, 66, 8]  |\n",
    "|    | [4, 2, 66, 8, 67]  |\n",
    "|    | [4, 2, 66, 8, 67, 68]  |\n",
    "|    | [4, 2, 66, 8, 67, 68, 69]  |\n",
    "|    | [4, 2, 66, 8, 67,68, 69, 70]  |\n",
    "\n",
    "Padded sequences \n",
    "\n",
    "| Line| Padded Input Sequences |\n",
    "|:----:|:----|\n",
    "| [4, 2 , 66, 8, 67, 68, 69, 70]   |   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0 , 4,2]  | \n",
    "|    | [0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 2, 66]  |\n",
    "|    | [0, 0, 0, 0, 0, 0, 0, 0, 4, 2, 66, 8]  |\n",
    "|    | [0, 0, 0, 0, 0, 0, 0, 4, 2, 66, 8, 67]  |\n",
    "|    | [0, 0, 0, 0, 0, 0, 4, 2, 66, 8, 67, 68]  |\n",
    "|    | [0, 0, 0, 0, 0, 4, 2, 66, 8, 67, 68, 69]  |\n",
    "|    | [0, 0, 0, 0, 4, 2, 66, 8, 67,68, 69, 70]  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A6NcMpdi92RY"
   },
   "source": [
    "### Training the prediction of word\n",
    "\n",
    "*   model.add(LSTM(20)) LSTM only take the earlier word forward, not take later word backward\n",
    "*   model.add(Bidirectional(LSTM(20)): more quicker to converge   \n",
    "\n",
    "The more words you predict, the more likely you get gibberish(non-sense). each word you predicted is based on probability, then next one less certain ...\n",
    "\n",
    "Use a large corpus for training will help.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X8deY9lmhLY_"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# max_sequence_len - 1, bc last word in each sentences as label\n",
    "model.add(Embedding(total_words,64, input_length = max_sequence_len - 1))\n",
    "model.add(LSTM(20)) #LSTM only take the earlier word forward, \n",
    "# model.add(Bidirectional(LSTM(20)) # add BRNN more quicker to converge   \n",
    "model.add(Dense(total_words, activation = 'softmax'))\n",
    "\n",
    "model.complie(loss = 'categorical_crossentropy', optmizer = 'adam', metrics = ['accuracy'])\n",
    "model.fit(xs, ys, epochs=500, verbose = 1)\n",
    "\n",
    "\n",
    "#*******************************************\n",
    "# Predict word\n",
    "#*******************************************\n",
    "seed_text = \"Laurence went to dublin\"  \n",
    "next_words = 10\n",
    "\n",
    "for_ in range(next_words):\n",
    "    token_list = tokenizer.text_to_sequences(seed_text)[0] #ignore Laurence, bc not in vocabulary\n",
    "    token_list = pad_sequences([token_list], maxlen = max_sequence_len - 1, padding = 'pre')\n",
    "    predicted = model.predict_classes(token_list, verbose = 0)\n",
    "    output_words = \"\"\n",
    "    for word, index in tokenziers.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            Break\n",
    "    seed_text =+= \" \" + output_word\n",
    "print(\"seed_text\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5xfMS2bOB1ZT"
   },
   "source": [
    "#### Using a new large dataset to train\n",
    "\n",
    "In colab using GPU, takes 20 minutes to train the model. Note: there is no line break in the prediction, have to add them manually to turn the word stream into poetry. \n",
    "\n",
    "if training a large number of corpus, maybe hit memory error, label text require many terabytes of RAM\n",
    "\n",
    "https://storage.googleapis.com/laurencemoroney-blog.appspot.com/irish-lyrics-eof.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YiKKXWw1B8XG"
   },
   "outputs": [],
   "source": [
    "#Download and Open dataset\n",
    "\n",
    "data = open('tmp/irish-lyrics-eof.txt').read()\n",
    "\n",
    "#update some parameter to work better for a large corpus\n",
    "# Change:\n",
    "#     1. 100 is  deminsion of embedding \n",
    "#     2. increase the number of output for LSTM\n",
    "#     3. Adam with setting learning rate\n",
    "#     4. Update training epochs for model.fit\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length = max_sequence_len - 1))\n",
    "model.add(Bidirectional(LSTM(150))) \n",
    "model.add(Dense(total_words, activation = 'softmax'))\n",
    "adam = Adam(lr=0.01)\n",
    "model.complie(loss = 'categorical_crossentropy', optmizer = adam, metrics = ['accuracy'])\n",
    "model.fit(xs, ys, epochs=100, verbose = 1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "NLP in TensorFlow Week4 Note.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
